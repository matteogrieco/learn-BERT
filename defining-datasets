import torch

MAX_LEN = 64
SEP_TOKEN_ID = 102

import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score

def compute_auc(y_true, y_pred):
    try:
        return roc_auc_score(y_true, y_pred)
    except ValueError:
        return np.nan

class QuestDataset(torch.utils.data.Dataset):
    def __init__(self, df, train_mode=True, labeled=True):
        self.df = df
        self.train_mode = train_mode
        self.labeled = labeled
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

    def __getitem__(self, index):
        row = self.df.iloc[index]
        token_ids, seg_ids = self.get_token_ids(row)
        if self.labeled:
            labels = self.get_label(row)
            return {'input_ids':token_ids, 'attention_mask':seg_ids, 'labels':labels}
        else:
            return {'input_ids':token_ids, 'attention_mask':seg_ids}

    def __len__(self):
        return len(self.df)

    def select_tokens(self, tokens, max_num):
        if len(tokens) <= max_num:
            return tokens
        if self.train_mode:
            num_remove = len(tokens) - max_num
            remove_start = random.randint(0, len(tokens)-num_remove-1)
            return tokens[:remove_start] + tokens[remove_start + num_remove:]
        else:
            return tokens[:max_num//2] + tokens[-(max_num - max_num//2):]

    def trim_input(self, title, question, max_sequence_length=MAX_LEN, 
                t_max_len=30, q_max_len=30):
        t = self.tokenizer.tokenize(title)
        q = self.tokenizer.tokenize(question)
        t_len = len(t)
        q_len = len(q)
        if t_len+q_len+3 > max_sequence_length:
            if t_max_len > t_len:
                t_new_len = t_len
                q_max_len = q_max_len + (t_max_len - t_len)
            else:
                t_new_len = t_max_len
            if q_max_len > q_len:
                q_new_len = q_len
            else:
                q_new_len = q_max_len
            t = t[:t_new_len]
            q = q[:q_new_len]
        return t, q
        
    def get_token_ids(self, row):
        t_tokens, q_tokens = self.trim_input(row.q1, row.q2)
        # print(t_tokens)

        tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]']
        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        
        seg_ids = torch.tensor([1]*len(token_ids)+[0] * (MAX_LEN - len(token_ids)))
        
        if len(token_ids) < MAX_LEN:
            token_ids += [0] * (MAX_LEN - len(token_ids))
        ids = torch.tensor(token_ids)
        
        return ids, seg_ids

    def get_label(self, row):
        return torch.tensor(row['label'].astype(int))

    def collate_fn(self, batch):
        token_ids = torch.stack([x[0] for x in batch])
        seg_ids = torch.stack([x[1] for x in batch])
    
        if self.labeled:
            labels = torch.stack([x[2] for x in batch])
            return token_ids, seg_ids, labels
        else:
            return token_ids, seg_ids
